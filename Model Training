import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestClassifier
# from xgboost import XGBRegressor
from sklearn import model_selection
from sklearn.utils import class_weight
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score



def TrainRandomForest(X_train, y_train, random_state):
    
    
    rf = RandomForestClassifier(random_state=random_state)

    #do a grid search over many parameters
    gridSearchParameters = {
        #'bootstrap': [True,False],
        'max_depth': [5, 10, 50, 100],
        'max_features': [3, 5, 10, 20],
        #'min_samples_leaf': [5, 25, 50, 100],
        #'min_samples_split': [5, 25, 50, 100], 
        'n_estimators': [5, 10, 20]
    }
    grid_search = model_selection.GridSearchCV(estimator = rf, param_grid = gridSearchParameters, cv = 5, n_jobs = -1, verbose = 2)
    grid_search.fit(X_train, y_train)
    
    print(grid_search.best_params_)
    print(grid_search.best_score_)
    print(grid_search.cv_results_)

    return grid_search.best_estimator_

    ### Take the best parameters from the grid search above (re-search if data changes):
    # rf = RandomForestRegressor(max_depth=5, max_features=5, n_estimators=100)
    # rf.fit(X_train, y_train)
    # return rf
    

# Test many different models - ToDo: IMPLEMENT LATER
def RunModels(X_train, y_train):
    dfs = []
    models = [
            ('LinReg', LinearRegression()), 
            # ('RF', RandomForestRegressor()),
            ('KNN', KNeighborsRegressor()),
            # ('XGB', XGBRegressor())
            ]
    results = []
    names = []
    scoring = ['r2','explained_variance','neg_mean_absolute_error','neg_mean_squared_error']
    for name, model in models:
        kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)
        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)
        #clf = model.fit(X_train, y_train)
        print(name)
        results.append(cv_results)
        names.append(name)
        this_df = pd.DataFrame(cv_results)
        print(this_df.head(100))
        this_df['model'] = name
        dfs.append(this_df)
        final = pd.concat(dfs, ignore_index=True)    
    
    return final

def PrintResults(y_pred, y_test):
    # The mean squared error
    print('Mean squared error: %.2f'
        % mean_squared_error(y_test, y_pred))
        # The mean squared error
    print('Mean absolute error: %.2f'
        % mean_absolute_error(y_test, y_pred))
    # The coefficient of determination: 1 is perfect prediction
    print('Coefficient of determination: %.2f'
        % r2_score(y_test, y_pred))
